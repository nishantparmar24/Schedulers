# -*- coding: utf-8 -*-
"""DataResource.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qi8I5Jh0jqDPfiFmof0omjtqKsQrdJZM

# Sentiment Analysis - Twitter

### Install required packages
"""

# !pip install searchtweets unidecode newsapi-python

"""### Import all required packages"""

from datetime import datetime as dt
from google.cloud import translate
from google.colab import drive
from html import unescape
from newsapi import NewsApiClient
from numpy import nan
from searchtweets import load_credentials, gen_rule_payload, collect_results
from time import sleep
from unidecode import unidecode
from urllib import parse
import json
import os
import pandas as pd
import re
import requests
import unicodedata

"""### Enable Google Drive in this notebook:"""

drive.mount('/gdrive')

DRIVE_LOCATION = "/gdrive/My Drive/Political Risk Project/Test Data"
SEARCH_RESULTS = "/gdrive/My Drive/Firebolt/Political Risk"
bckp_loc = "/gdrive/My Drive/Political Risk Project/Test Data/Backup"

search_results = os.path.join(SEARCH_RESULTS, 'new_outputs.csv')
tw_output_directory = os.path.join(DRIVE_LOCATION, 'Twitter Results')
nh_output_directory = os.path.join(DRIVE_LOCATION, 'News Results')
Twitter_API_usage_file = os.path.join(tw_output_directory, 'api_usage.json')
News_API_usage_file = os.path.join(nh_output_directory, 'api_usage.json')

"""### Fetch Google Translate API credentials"""

translate_api_dir = "/gdrive/My Drive/Firebolt/API Keys"
translate_api_key = os.path.join(translate_api_dir, 'eiu-searchautomation.json')
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = translate_api_key
try:
    translate_client = translate.Client()
    
    def translate_keyword(keyword, target_lang="en"):
        response = translate_client.translate(keyword,
                                              target_language=target_lang)
        if response:
            return response["translatedText"]
        else:
            return ""
    
    def detect_language(text_string):
        result = translate_client.detect_language(text_string)
        return result["language"]
    
except Exception as e:
    print("Exception occurred while instantiating \
    Google Cloud Translate: {}".format(e))

"""### Get text with the least length from a collection of text values"""

def phrase_min_length(text_sample):
    sample_df = pd.DataFrame(text_sample, columns=['text'])
    sample_df["length"] = [l for l in map(len, text_sample)]
    min_pos = sample_df["length"] == sample_df["length"].min()
    min_phrase = sample_df[min_pos]["text"]

"""### Function to remove emojis from a text string"""

def remove_emojis(input_string):
    return_string = ""

    for character in input_string.strip():
        try:
            character.encode("ascii")
            return_string += character
        except UnicodeEncodeError:
            replaced = unidecode(str(character))
            if replaced not in ['', '[?]']:
                return_string += replaced
    return return_string.strip()

"""### Function to extract (and segregate) tweet-components: links, user-handles and hashtags"""

def extract_components(clean_string):
    token_list = re.split(r"\s+", clean_string.strip())
    url_ptrn = '^https?:\/\/.*[\r\n]*'
    handle_ptrn = '^\@.*'
    trend_ptrn = '^#.*'
    url_collection = list()
    handle_collection = list()
    trend_collection = list()
    tokens = list()
    for token in token_list:
        # print("token = {}".format(token))
        url_match = re.search(url_ptrn, token) or re.match(url_ptrn, token)
        if url_match:
            # print("\t\tURL Pattern matched")
            url_collection.append(
                token) if token not in url_collection else False
            ix = token_list.index(token)
            token_list[ix] = ''
            continue
        handle_match = re.search(
            handle_ptrn, token) or re.match(handle_ptrn, token)
        if handle_match:
            # print("\t\tHandle Pattern matched")
            handle_collection.append(
                token) if token not in handle_collection else False
            ix = token_list.index(token)
            token_list[ix] = ''
            continue
        trend_match = re.search(
            trend_ptrn, token) or re.match(trend_ptrn, token)
        if trend_match:
            # print("\t\tTrend Pattern matched")
            trend_collection.append(
                token) if token not in trend_collection else False
            ix = token_list.index(token)
            token_list[ix] = ''
            continue
        if token.strip():
            tokens.append(token)
    del token_list
    return (tokens, url_collection, handle_collection, trend_collection)

"""### Function to clean and structure the text data"""

def structure_text(entire_str):
    separations = dict()
    clean_str = remove_emojis(entire_str)
    tokens, links, handles, tags = extract_components(clean_str)
    clean_str = (" ".join(tokens)).strip()
    print("clean_str = {}".format(clean_str))
    cleaner_str = (re.subn("[:-]+", "", clean_str))[0].strip()
    try:
        cleaner_str = unescape(translate_keyword(cleaner_str))
    except Exception as e:
        print("Exception occurred: {}".format(e))
        sleep(5)
        try:
            cleaner_str = unescape(translate_keyword(cleaner_str))
        except Exception as e:
            cleaner_str = ""
    print("cleaner_str = {}".format(cleaner_str))
    separations['text'] = cleaner_str
    separations['links'] = ",".join(links)
    separations['handles'] = ",".join(handles)
    separations['tags'] = ",".join(tags)
    return separations

"""### Function to include a zero for date values less than 10"""

format_date_str = lambda s: "0{}".format(s) if int(s) < 10 else "{}".format(s)

"""### Function to return a formatting of current datetime (for file names)"""

today = lambda: "{}-{}-{}_{}-{}".format(dt.now().year,
                                format_date_str(dt.now().month),
                                format_date_str(dt.now().day),
                                format_date_str(dt.now().hour),
                                format_date_str(dt.now().minute))
today_date = today().split("_")[0]
today_date

"""### Function to format datetime as string (time separated with a ":")"""

def extract_datetime_string(datetime_obj):
    if datetime_obj:
        return "{}-{}-{} {}:{}:{}".format(datetime_obj.year,
                                          format_date_str(datetime_obj.month),
                                          format_date_str(datetime_obj.day),
                                          format_date_str(datetime_obj.hour),
                                          format_date_str(datetime_obj.minute),
                                          format_date_str(datetime_obj.second))

"""### Functions to create and validate custom dates"""

def get_month_days_map(year, month=None):
    month_days_map = {(1, 3, 5, 7, 8, 10, 12): 31, (4, 6, 9, 11): 30}
    if year % 4 == 0:
        month_days_map.update({(2,): 29})
    else:
        month_days_map.update({(2,): 28})
    month = 12 if month == 0 else month
    if month:
        for months, n_days in month_days_map.items():
            if month in months:
                return n_days
    return month_days_map

def validate_custom_date(year, month, day):
    print("Date entered: {}-{}-{}".format(year, 
                                          format_date_str(month), 
                                          format_date_str(day)))
    n_days = get_month_days_map(year, month=month)
    if month not in range(1, 13):
        print("Invalid month entered!")
        return False
    if day not in range(1, n_days+1):
        print("Invalid day entered!")
        return False
    if year > dt.now().year and month > dt.now().month and day > dt.now().day:
        print("Wrong date entered! Please enter correct date values.")
        return False
    elif year > dt.now().year:
        print("Invalid year entered!")
        return False
    elif year == dt.now().year:
        if month > dt.now().month:
            print("Invalid month entered!")
            return False
        if day > dt.now().day:
            print("Invalid day entered!")
            return False
    return True

def get_custom_date(year=dt.now().year, month=dt.now().month, day=dt.now().day):
    date_valid = validate_custom_date(year, month, day)
    custom_date = ""
    if date_valid:
        from_day = diff_day = day - 7
        from_mon = month
        from_year = year
        prev_n = get_month_days_map(year, month - 1)
        if diff_day < 0:
            from_day = diff_day % prev_n
            from_mon = month - 1 if month != 1 else 12 
            from_year = year - 1 if month == 1 else year
        elif diff_day == 0:
            from_day = prev_n
            from_mon = month - 1 if month != 1 else 12
            from_year = year - 1 if month == 1 else year
        print("from_day = {}".format(from_day))
        custom_date = "{}-{}-{}".format(from_year, 
                                        format_date_str(from_mon), 
                                        format_date_str(from_day)) 
    return custom_date

get_custom_date()

"""### Fetch Twitter Premium Search API credentials"""

creds_file = '/gdrive/My Drive/Political Risk Project/TwitterCredentials/Bckp/twitter_creds.yaml'
search_tweets_api = 'search_tweets_30_day_dev'
premium_search_args = load_credentials(filename=creds_file,
                                       yaml_key=search_tweets_api,
                                       env_overwrite=False)

"""### Defining all the filters, queries and user handles"""

it_query_filter = "-has:media place_country:IT"
it_en_query_filter = "-has:media place_country:IT lang:en"
generic_query_filter = "-has:media"
queries = ["early election", "snap election", "government collapse", 
           "government coalition", "election", "instability", "uncertainty",
           "crisis", "coalition"]
queries_it = [q for q in map(lambda s: translate_keyword(s, "it"), queries)]

from_users = ["lorepregliasco", "FerdiGiugliano", "AlbertoNardelli", 
              "gavinjones10"]

# print(queries)
# print(queries_it)

"""### Twitter API Usage Monitor"""

def get_api_usage(date):
    if os.path.lexists(Twitter_API_usage_file):
        api_usage = pd.read_json(Twitter_API_usage_file, orient='records')
        print(api_usage)
        print(list(api_usage.columns))
        if date in api_usage.columns:
            return api_usage, api_usage[date].squeeze()
        else:
            # update_api_count(date, 0)
            return api_usage, 0
    return None, None


def update_api_count(date, count_add):
    api_usage, count = get_api_usage(date)
    if not api_usage.empty:
        updated_count = 0
        print(list(api_usage.columns))
        if date in api_usage.columns:
            print("Month present\n")
            api_usage[date] += count_add
            updated_count = api_usage[date].squeeze()
        else:
            print("Month absent\n")
            api_usage[date] = 1
        print("api_usage = {}".format(api_usage[date]))
        api_usage.to_json(Twitter_API_usage_file, 
                          orient='records', 
                          date_format='iso')
        sleep(3)
        print("Updating new API usage...")
        _, new_count = get_api_usage(date)
        if new_count != updated_count:
            print("...")
            drive.mount("/gdrive", force_remount=True)
        _, new_count = get_api_usage(date)
        # print(new_count)
        # while new_count != updated_count:
        #     print("Updating new API usage...")
        #     _, new_count = get_api_usage(date)
        print("API usage count updated! Current usage: {}".format(new_count))

"""### NewsAPI Usage Monitor"""

def get_news_api_usage(date):
    if os.path.lexists(News_API_usage_file):
        api_usage = pd.read_json(News_API_usage_file, orient="records")
        if date in api_usage.columns:
            return api_usage, api_usage[date].squeeze()
        else:
            # update_news_api_count(date)
            return api_usage, 1
    return None, None


def update_news_api_count(date):
    api_usage, count = get_news_api_usage(date)
    if not api_usage.empty:
        updated_count = 0
        if date in api_usage.columns:
            api_usage[date] += 1
            updated_count = api_usage[date].squeeze()
        else:
            api_usage[date] = 0
        api_usage.to_json(News_API_usage_file, orient="records", date_format="iso")
        sleep(3)
        _, new_count = get_news_api_usage(date)
        print("Updating new API usage...")
        # print(new_count)
        if new_count != updated_count:
            print("...")
            drive.mount("/gdrive", force_remount=True)
        _, new_count = get_news_api_usage(date)
        print("API usage count updated! Current usage: {}".format(new_count))

"""### Fetch NewsAPI Credentials"""

def get_api_key():
    src_ = "/gdrive/My Drive/Political Risk Project/NewsAPIKey/"
    with open(os.path.join(src_, "news_api_key.json")) as key:
        obj = json.load(key)
        return obj["api_key"]


newsapi = NewsApiClient(api_key=get_api_key())

"""### Initialize News API variables"""

news_sources = ",".join(["reuters", "ansa", "google-news-it"])
print(news_sources)
# queries = ["early election", "snap election", "government collapse", 
#            "government coalition", "election", "instability", "uncertainty",
#            "crisis", "coalition"]
country = "it"
page_size = 100
endpoint = "everything" # "top-headlines"

"""### Fetch News Headlines using the News API"""

def fetch_news(queries, sources, page_size=100, **kwargs):
    top_headlines_map = dict()
    # saved_files = list()
    news_collection = pd.DataFrame()
    for query in queries:
        headlines = newsapi.get_everything(q=query,
                                           sources=sources,
                                           sort_by="relevancy",
                                           page_size=page_size)
        if headlines["totalResults"] > 0:
            # response_text = json.loads(headlines.text)
            all_articles = headlines["articles"]
            collection = list()
            for article in all_articles:
                data_ = dict()
                source = article.pop("source")
                content = article.pop("content")
                source = {"source": source["id"]}
                article.update(source)
                collection.append(article)
            data_df = pd.DataFrame(data=collection)
            if not data_df.empty:
                match_str = "({})".format(query)
                reqd_df = data_df[data_df["description"].str.contains(
                    match_str)]
                if news_collection.empty:
                    news_collection = reqd_df
                else:
                    news_collection = news_collection.append(reqd_df)
                # responses_file = "{}_on_{}.csv".format(endpoint,
                #                                       query.replace(" ", "_"))
                # top_headlines_map[query] = reqd_df
            
                # out_file = os.path.join(output_directory, responses_file)
                # data_df.to_csv(out_file, index=False)
            update_news_api_count(today())
    return news_collection

"""### Functions to fetch tweets by querying the Twitter API with the given queries and filters"""

def get_user_queries_filter(queries, from_users):
    user_queries_filter = '("{}") from:{} -has:media'
    user_queries_list = list()
    for query in queries:
        for user in from_users:
            filter_ = user_queries_filter.format(query, user)
            user_queries_list.append(filter_)
    return user_queries_list


def get_user_specific_filter(from_users):
    user_specific_filter =  'from:{} -has:media'
    user_specific_list = list()
    # for query in queries:
    for user in from_users:
        filter_ = user_specific_filter.format(user)
        user_specific_list.append(filter_)
    return user_specific_list


def get_tweets(query_set, query_filter=None):
    tweets_list = list()
    for query in query_set:
        curr_month = "{}-{}".format(dt.now().year, 
                                    format_date_str(dt.now().month))
        _, curr_usage = get_api_usage(curr_month)
        if curr_usage >= 24999:
            print("Twitter API limit is about to exceed! Returning now ...\n")
            break
        if query_filter:
            q = '("{}") {}'.format(query, query_filter)
        else:
            q = "{}".format(query)
            print("No filter/Filter in query_set: {}".format(q))
        print("Collecting for {}".format(q))
        try:
            rule = gen_rule_payload(q, results_per_call=100)
            tweets = collect_results(rule,
                                    max_results=100,
                                    result_stream_args=premium_search_args)
            print(len(tweets))
            update_api_count(curr_month, len(tweets))
            tweets_list.append(tweets)

        except Exception as e:
            print("Exception occurred while fetching tweets: {}".format(e))
            break
    return tweets_list

user_specific_queries = get_user_specific_filter(from_users)
user_specific_queries

"""### Process the tweets to produce a collection of tweet-text, hashtags, links, and the date-time when they were created."""

def process_tweets(tweets_list):
    tweets_collection = pd.DataFrame()
    for tweets in tweets_list:
        for tweet in tweets:
            structured_tweets = structure_text(tweet.all_text)
            structured_tweets.update({"created_time":tweet.created_at_datetime})
            reqd_df = pd.DataFrame([structured_tweets])

            if tweets_collection.empty:
                tweets_collection = reqd_df
            else:
                tweets_collection = tweets_collection.append(reqd_df)
    return tweets_collection

"""### To save the output of the tweet-information into the categories, General or Italy"""

def save_output(tweets_collection, 
                curr_datetime,
                category="General", 
                subset="predictions"):
    if category in ["General", "Italy"]:
        file_name = 'relevant_{}_{}.csv'.format(subset, curr_datetime)
        tweets_collection.to_csv(
            os.path.join(tw_output_directory, category, file_name),
            index=False)
    else:
        print("Output Directory is not valid!")

def get_saved_versions(name, location):
    files = os.listdir(location)
    found_files = list()
    for f in files:
        match = re.search("({})\-(\d+)\.csv$".format(name), f)
        if match:
            f_groups = match.groups()
            saved_version = f_groups[-1]
            map_ = {"file": f, "version": int(saved_version)}
            found_files.append(map_)
    if found_files:
        versions_ = pd.DataFrame(found_files)
        return versions_
    else:
        return pd.DataFrame()


def save_output_version(data, name, location, version=None):
    filename_format = "{name}-{version}.csv"
    next_version = int(version) if version else 1
    all_versions = get_saved_versions(name, location)
    if not all_versions.empty:
        next_version = 1 + int(all_versions["version"].max())
        if version and version >= next_version:
            next_version = version
    new_name = filename_format.format(name=name, version=next_version)
    data.to_csv(os.path.join(location, new_name), index=False)
    return new_name

# get_saved_versions("it-it-tweets", os.path.join(tw_output_directory, "Italy"))
get_saved_versions("en-it-news", nh_output_directory)

"""## Deployed model

### Endpoint URL to query the model in real time
"""

enpoint_uri = "http://ac6a2064dee3c11e99ced0a13821e56d-733867741.ap-southeast-1.elb.amazonaws.com/sentiment/classifier"
headers = {"content-type": "application/json"}

"""### Get polarity of a piece of text"""

def get_polarity(sentence):
    data = json.dumps({"polarity": sentence})
    response = requests.post(enpoint_uri, data=data, headers=headers)
    return response.json() if response.status_code == 200 else ""

"""## Get and Process Tweets from a set of users

### Get tweets posted by a list of users
"""

user_specific_tweets = get_tweets(query_set=user_specific_queries)

"""### Process the tweets"""

user_tweets_collection = process_tweets(user_specific_tweets)
user_tweets_collection

"""### Clean the DataFrame of tweets"""

user_tweets_collection = user_tweets_collection.reset_index().drop(columns=['index'])
duplicacy_subset = list(set(user_tweets_collection.columns) - set(["created_time"]))
user_tweets_collection.drop_duplicates(subset=duplicacy_subset, inplace=True)
user_tweets_collection.dropna(subset=["text"])
# user_tweets_collection.dropna(subset=["text"])

"""## Get and Process Tweets from Italy

### Clean the DataFrame of tweets from Italy
"""

italy_tweets_collection = italy_tweets_collection.reset_index().drop(columns=['index'])
duplicacy_subset = list(set(italy_tweets_collection.columns) - set(["created_time"]))
italy_tweets_collection.drop_duplicates(subset=duplicacy_subset, inplace=True)
italy_tweets_collection

save_output_version(data=italy_tweets_collection,
                    name="en-it-tweets",
                    location=tw_output_directory)

"""### Attach predictions of the tweets into the DataFrame"""

italy_tweets_collection["polarity_v1"] = italy_tweets_collection["text"].apply(get_polarity)

italy_tweets_collection

"""### Save output of the collection of tweets in Google Drive
Folder location: https://drive.google.com/open?id=1iX7i_jarE7hhOVf68p-KzUECrEG59u4T
"""

save_output(italy_tweets_collection, 
            category="Italy",
            subset="predictions", 
            curr_datetime=today())

"""## Get generic tweets (relevant to the keywords)

### Get all tweets that contain the keywords like *snap election*, *early election*, *political instability*
"""

generic_tweets = get_tweets(queries, generic_query_filter)

"""### Process all the generic tweets"""

generic_tweets_collection = process_tweets(generic_tweets)

"""### Clean the DataFrame of generic tweets"""

generic_tweets_collection = generic_tweets_collection.reset_index().drop(columns=['index'])
duplicacy_subset = list(set(generic_tweets_collection.columns) - set(["created_time"]))
generic_tweets_collection.drop_duplicates(subset=duplicacy_subset, inplace=True)
generic_tweets_collection

"""### Save the generic Twitter results to Google Drive"""

save_output(generic_tweets_collection, subset="tweets", curr_datetime=today())

"""### Merge the predictions of tweets in the DataFrame"""

generic_tweets_collection["polarity_v1"] = generic_tweets_collection["text"].apply(get_polarity)

"""### Save the predicted generic tweets in Google Drive
Folder location: https://drive.google.com/open?id=13FRvvDetM4Bcsdw9dblKuEr9Tyz219_A
"""

save_output(generic_tweets_collection, 
            subset="predictions", 
            curr_datetime=today())

"""## Fetch tweets (en-it, it-it)

### Get and save tweets specific to the country of origin (English)
"""

en_it_tweets = get_tweets(queries, it_query_filter)
en_it_tweets_collection = process_tweets(en_it_tweets)
en_it_tweets_collection = en_it_tweets_collection.reset_index().drop(columns=['index'])
duplicacy_subset = list(set(en_it_tweets_collection.columns) - set(["created_time"]))
en_it_tweets_collection.drop_duplicates(subset=duplicacy_subset, inplace=True)
en_it_tweets_collection

save_output_version(data=en_it_tweets_collection,
                    name="en-it-tweets",
                    location=tw_output_directory)

"""### Get and save tweets from Italy (Italian)"""

it_it_tweets = get_tweets(queries_it, it_query_filter)
it_it_tweets_collection = process_tweets(it_it_tweets)
it_it_tweets_collection = it_it_tweets_collection.reset_index().drop(columns=['index'])
duplicacy_subset = list(set(it_it_tweets_collection.columns) - set(["created_time"]))
it_it_tweets_collection.drop_duplicates(subset=duplicacy_subset, inplace=True)
it_it_tweets_collection

save_output_version(data=it_it_tweets_collection,
                    name="it-it-tweets",
                    location=os.path.join(tw_output_directory, "Italy"))

"""## Collect Data

### Correct Headers of files with inconsistent column names
"""

def file_valid(file_path):
    if os.path.isfile(file_path):
        if re.search(".*\.csv$", file_path):
            return file_path
    return ""


def correct_files(file_collection):
    for filename in file_collection:
        if filename:
            df = pd.read_csv(filename, encoding="utf-8")
            df.rename(columns={df.columns[0]: "text"}, 
                    inplace=True)
            df.to_csv(filename, index=False)


files_to_correct = [f for f in map(lambda s:
    file_valid(os.path.join(tw_output_directory, s)),
    os.listdir(tw_output_directory))]

correct_files(files_to_correct)

# file_ = pd.read_csv(os.path.join(tw_output_directory, 
#                                  "relevant_tweets_output_2.csv"),
#                     encoding="utf-8")
# file_.rename(columns={"headline_text": "text"}, inplace=True)
# file_

"""### Fetch en-it News Headlines"""

en_fresh_headlines = fetch_news(queries, news_sources)
if not en_fresh_headlines.empty:
    en_news_collection = en_fresh_headlines.reset_index().drop(columns=["index"])
    en_news_collection.to_csv(os.path.join(nh_output_directory, 
                                        "news_{}.csv".format(today())),
                            index=False)
    print("en_news_collection")
    print(en_news_collection.columns)

save_output_version(data=en_news_collection, 
                    name="en-it-news", 
                    location=nh_output_directory)

"""### Fetch it-it News Headlines"""

fresh_headlines = fetch_news(queries_it, news_sources)
if not fresh_headlines.empty:
    it_news_collection = fresh_headlines.reset_index().drop(columns=["index"])
    it_news_collection.to_csv(os.path.join(nh_output_directory, 
                                        "news_{}.csv".format(today())),
                            index=False)
    print("it_news_collection")
    print(it_news_collection.columns)

save_output_version(data=it_news_collection, 
                    name="it-it-news", 
                    location=nh_output_directory)

"""### Collect and combine previously saved and fresh data from Twitter and NewsAPI (and also from Firebolt)"""

def get_previous_data(file_loc, column_list=None):
    # file_name = ""
    # file_loc = os.path.join(DRIVE_LOCATION, PREV_DATA_DIR, file_name)
    prev_data = pd.read_csv(file_loc)
    if not prev_data.empty:
        return prev_data[column_list] if column_list else prev_data
    return None


def get_fresh_data(tweets, headlines):
    processed_tweets = None
    news_collection = None
    if tweets:
        fresh_data = dict()
        # get fresh tweets:
        # tweets = get_tweets(queries, query_filter)
        processed_tweets = process_tweets(tweets)
        processed_tweets = processed_tweets.reset_index().drop(
            columns=['index'])
        print("processed_tweets")
        print(processed_tweets.columns)
        duplicacy_subset = list(
            set(processed_tweets.columns) - {"created_time"})
        processed_tweets.drop_duplicates(subset=duplicacy_subset, inplace=True)
        processed_tweets.to_csv(os.path.join(tw_output_directory, 
                                             "tweets_{}.csv".format(today())),
                                index=False)
    # get fresh news headlines:
    # headlines = fetch_news(queries, sources, ...)
    if not headlines.empty:
        news_collection = headlines.reset_index().drop(columns=["index"])
        news_collection.to_csv(os.path.join(nh_output_directory, 
                                            "news_{}.csv".format(today())),
                               index=False)
        print("news_collection")
        print(news_collection.columns)
    fresh_data["tweets"] = processed_tweets
    fresh_data["headlines"] = news_collection
    return fresh_data


def frame_files(files):
    final_df = pd.DataFrame()
    for f in files:
        if os.path.exists(f) and os.path.isfile(f):
            inter_df = pd.read_csv(f, encoding="UTF-8")
            final_df = final_df.append(inter_df)
    return final_df


def combine_data_offline(tweets_files, news_files, prev_files_map=None):
    processed_headlines = processed_tweets = prev_data_df = pd.DataFrame()
    # tweets_files = "it-it-tweets" if not tweets_files else tweets_files
    # news_files = "it-it-news" if not news_files else news_files

    if prev_files_map:
        for filename, column in prev_files_map.items():
            file_df = get_previous_data(filename, column)
            print("prev_data = {}".format(file_df.shape))
            if prev_data_df.empty:
                prev_data_df = file_df
            else:
                prev_data_df = prev_data_df.append(file_df)
        print("combined prev_data = {}".format(prev_data_df.shape))
        prev_data_df = prev_data_df.reset_index().drop(columns=["index"])
        print("post-reset prev_data = {}".format(prev_data_df.shape))
        prev_data_df.rename(columns={prev_data_df.columns[0]: "text"}, 
                            inplace=True)
        prev_data_df.drop_duplicates(inplace=True)
        print("post-removing duplicates prev_data = {}".format(prev_data_df.shape))
    print("final prev_data = {}".format(prev_data_df.shape))
    
    tw_text = nh_text = list()
    saved_tweets = get_saved_versions(tweets_files["type"], 
                                      tweets_files["path"])
    print("\nFetching it-it Tweets")
    if not saved_tweets.empty:
        tweets_ = list(saved_tweets["file"])
        processed_tweets = pd.DataFrame()
        for tweet_ in tweets_:
            print("reading {}".format(os.path.join(tweets_files["path"], tweet_)))
            df_ = pd.read_csv(os.path.join(tweets_files["path"], tweet_),
                              encoding="UTF-8")
            print("{}: shape = {}".format(tweet_, df_.shape))
            processed_tweets = processed_tweets.append(df_)
        if not processed_tweets.empty:
            tw_text = list(processed_tweets["text"])
    print("len(tw_text): {}".format(len(tw_text)))
    
    saved_news = get_saved_versions(news_files["type"], news_files["path"])
    print("\nFetching it-it News Headlines")
    if not saved_news.empty:
        news_ = list(saved_news["file"])
        processed_headlines = pd.DataFrame()
        for headline_ in news_:
            print("reading {}".format(os.path.join(news_files["path"], headline_)))
            df_ = pd.read_csv(os.path.join(news_files["path"], headline_),
                              encoding="UTF-8")
            print("{}: shape = {}".format(headline_, df_.shape))
            processed_headlines = processed_headlines.append(df_)
        if not processed_headlines.empty:
            nh_text = list(processed_headlines["url"])
    print("len(nh_text): {}".format(len(nh_text)))
    tw_text.extend(nh_text)
    if not prev_data_df.empty:
        base_ = list(prev_data_df["text"])
        base_.extend(tw_text)
    else:
        base_ = tw_text
    print("len(base_): {}".format(len(base_)))
    final_df = pd.DataFrame(base_, columns=["data"])
    final_df.drop_duplicates(inplace=True)
    return final_df


def combine_data(prev_files):
    prev_data_df = pd.DataFrame()
    for filename, column in prev_files.items():
        file_df = get_previous_data(filename, column)
        if prev_data_df.empty:
            prev_data_df = file_df
        else:
            prev_data_df = prev_data_df.append(file_df)
    prev_data_df = prev_data_df.reset_index().drop(columns=["index"])
    prev_data_df.rename(columns={prev_data_df.columns[0]: "text"}, inplace=True)
    prev_data_df.drop_duplicates(inplace=True)
    # return prev_data_df
    # get fresh data
    twitter_keywords = queries
    selected_filter = it_query_filter
    tweets = get_tweets(twitter_keywords, selected_filter)
    news_keywords = twitter_keywords
    # news_sources = []
    headlines = fetch_news(news_keywords, news_sources)
    fresh_data = get_fresh_data(tweets, headlines)
    combined_data = prev_data_df.append(fresh_data["tweets"]["text"]).append(
        fresh_data["headlines"]["url"])
    return combined_data

tweets_files_it = {"type": "it-it-tweets", 
                "path": os.path.join(tw_output_directory, "Italy")}
news_files_it = {"type": "it-it-news", 
                "path": nh_output_directory}
search_results_path = os.path.join(DRIVE_LOCATION, 
                                   "Search-Results-{}.csv".format(
                                       today().split("_")[0]))
search_results_map = {search_results_path: "link"}
search_combined_it_it = combine_data_offline(tweets_files_it, 
                                              news_files_it,
                                              search_results_map)
search_combined_it_it["rating"] = ""

search_combined_it_it.to_csv(
    os.path.join(DRIVE_LOCATION, "it-it", "Data-Collection-{}-v1.csv".format(
        today_date
    )), 
    index=False)

tweets_files_it = {"type": "it-it-tweets", 
                "path": os.path.join(tw_output_directory, "Italy")}
news_files_it = {"type": "it-it-news", 
                "path": nh_output_directory}
offline_combined_it_it = combine_data_offline(tweets_files_it, news_files_it)

offline_combined_it_it.to_csv(
    os.path.join(DRIVE_LOCATION, "it-it", "Data-Collection-{}.csv".format(
        today_date
    )), 
    index=False)

tweets_files_en = {"type": "en-it-tweets", 
                "path": tw_output_directory}
news_files_en = {"type": "en-it-news", 
                "path": nh_output_directory}
offline_combined_en_it = combine_data_offline(tweets_files_en, news_files_en)
offline_combined_en_it

offline_combined_en_it.to_csv(
    os.path.join(DRIVE_LOCATION, "en-it", "Data-Collection-{}-v2.csv".format(
        today().split("_")[0]
    )), 
    index=False)

# prev_files = dict()
# prev_files.extend(os.listdir(nh_output_directory))

def get_prev_files():
    dir_col_map = {tw_output_directory: "text",
                   nh_output_directory: "url",
                   search_results: "link"
                   }
    prev_files = dict()

    def extract_csv_files(directory, source):
        # files_ = list()
        if os.path.isdir(directory):
            for part in os.listdir(directory):
                # print(part)
                if os.path.isdir(os.path.join(directory, part)):
                    extract_csv_files(os.path.join(directory, part), source)
                else:
                    if re.search(".*\.csv$", part) and \
                    part not in prev_files.keys():
                        prev_files[os.path.join(directory, part)] = source
        elif os.path.isfile(directory):
            if re.search(".*\.csv$", directory) \
            and directory not in prev_files.keys():
                prev_files[directory] = source
    
    for directory, column in dir_col_map.items():
        extract_csv_files(directory, column)
    
    return prev_files


get_prev_files()
# extract_csv_files(tw_output_directory, "text")
# extract_csv_files(nh_output_directory, "url")
# extract_csv_files(search_results, "link")
# print(prev_files)
# combine_data(prev_files)

"""### Combine Offline Data"""

# text_data = combine_data_offline(prev_files)
# collection = pd.DataFrame(text_data)
# collection["rating"] = ""

collection.drop_duplicates(subset=["data"], inplace=True)

collection.to_csv(os.path.join(
    DRIVE_LOCATION, "DataCollection_{}.csv".format(today())), 
                  index=False)

"""## Redundant cells:"""

# tweets_data = pd.DataFrame(tweets)
# tweets_data
# print(tweets_data.loc[0]["text"])

# all_texts = [{"all_text": structure_text(t.all_text), 
#               "created_at": extract_datetime_string(t.created_at_datetime)}
#               for t in tweets]
# all_texts
# for tweet in tweets:
#     print(dir(tweet))

# str_ = ["Beyonc√© necesita ver esto. Que diosa @TiniStoessel üî•üî•üî• https://t.co/gadVJbehQZ", 
#         "When Beyonc√© adopts a dog üôåüèæ https://t.co/U571HyLG4F",
#         "Yup!!!! ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è #davechappelle https://t.co/ybSGNrQpYF"]

# c = [clean_str for clean_str in map(remove_emojis, str_)]
# s1 = "Beyonc√© necesita ver esto. Que diosa"
# s1 = "Yup!!!! ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è #davechappelle #love https://t.co/ybSGNrQpYF"
# s1 += "@davarch1 @PKBook22 @davarch1 @TheStephenRalph https://t.co/gadVJbehQZ https://t.co/gadVJbehQZ https://t.co/U571HyLG4F"

# # detect_language(" ".join(['Un', 'voto', 'importante', 'con', 'incognite']))
# s1 = "Un voto importante con incognite di coalizione I saw this on the BBC and thought you should see it: votes in snap election after video sting scandal"
# # translate_keyword(" ".join(['Un', 'voto', 'importante', 'con', 'incognite']), "en")
# s1_arr = s1.split(" ")
# non_english = dict()
# for index, token in enumerate(s1_arr):
#     if detect_language(token) != "en":
#         non_english.update({index: token})

# # remaining_eng = s1_arr[list(non_english.keys())[-1]+1:]
# non_eng_text = " ".join(list(non_english.values()))
# translated_non_eng = translate_keyword(non_eng_text)
# s1t = s1.replace(non_eng_text, translated_non_eng)
# # list(non_english.keys())
# italian_tweets = pd.DataFrame(all_texts)
# italian_tweets
# # italian_tweets.to_csv(os.path.join(output_directory, 
# #                                    "italian_tweets_{}.csv".format(today)),
# #                       index=False)

# text_ = """This piece of text is taken from http://ansa.it. Una lunga maratona notturna sblocca l'impasse del governo sulla legge di bilancio e il decreto fiscale. Arriva il via libera salvo intese. Alle cinque del mattino, dopo un Consiglio dei ministri di quasi sei ore, il premier Giuseppe Conte e il ministro dell'Economia Roberto Gualtieri si mostrano stanchi ma soddisfatti: arriva una manovra da circa 30 miliardi, con lo stop all'aumento dell'Iva, tre miliardi per tagliare le tasse ai lavoratori, 600 milioni per la famiglia, la fine del superticket da settembre 2020 e il piano di lotta all'evasione Italia cashless voluto da Conte."""
# sample_text = re.split(r'\.\s+', text_)

# clean_tweets = [row for row in map(structure_text, sample_text)]
# # for tweet_row in clean_tweets:
# #     ix = clean_tweets.index(tweet_row)
# #     text = tweet_row["text"]
# #     lang = detect_language(text)
# #     if lang != "en":
# #         translated_text = unescape(
# #             translate_keyword(text, "en"))
# #         clean_tweets[ix]["text"] = translated_text
# pd.DataFrame(clean_tweets, index=None)

# # query = '("early election" OR "snap election OR government collapse") {}'.format(query_filter)
# query_2 = '("government collapse") {}'.format(query_filter)
# # query_url = parse.quote(query)

# def structure_text(entire_str):
#     separations = dict()
#     clean_str = remove_emojis(entire_str)
#     tokens, links, handles, tags = extract_components(clean_str)
#     clean_str = (" ".join(tokens)).strip()
#     print("clean_str = {}".format(clean_str))
#     cleaner_str = (re.subn("[:-]+", "", clean_str))[0].strip()
#     cleaner_str_split = cleaner_str.split(" ")
#     non_english = dict()
#     try:
#         for index, token in enumerate(cleaner_str_split):
#             if detect_language(token) != "en":
#                 non_english.update({index: token})
#         if non_english:
#             print("non_english: {}".format(non_english))
#             non_eng_text = " ".join(list(non_english.values()))
#             translated_non_eng = unescape(translate_keyword(non_eng_text))
#             cleaner_str = cleaner_str.replace(non_eng_text, translated_non_eng)
#     except Exception as e:
#         print("Exception occurred: {}".format(e))
#         cleaner_str = ""
#     print("cleaner_str = {}".format(cleaner_str))
#     separations['text'] = cleaner_str
#     separations['links'] = ",".join(links)
#     separations['handles'] = ",".join(handles)
#     separations['tags'] = ",".join(tags)
#     return separations

# def process_tweets(tweets_list):
#     tweets_collection = pd.DataFrame()
#     for tweets in tweets_list:
#         for tweet in tweets:
#             structured_tweets = structure_text(tweet.all_text)
#             structured_tweets.update({"created_time":tweet.created_at_datetime})
#             reqd_df = pd.DataFrame([structured_tweets])
#             # reqd_df.drop_duplicates(inplace=True)
#             # reqd_df["text"] = reqd_df["text"].apply(
#             #     lambda s: re.subn('(\s)', ' ', s)[0])
#             # reqd_df["text"] = reqd_df["text"].apply(structure_text)
#             # match_str = '({})'.format(query)
#             if tweets_collection.empty:
#                 tweets_collection = reqd_df
#             else:
#                 tweets_collection = tweets_collection.append(reqd_df)
#     # tweets_collection.drop_duplicates(inplace=True)
#     return tweets_collection

# rule = gen_rule_payload(query, results_per_call=100)
# r = gen_rule_payload(query_2, results_per_call=100)

# tweets_2 = collect_results(r,
#                          max_results=500,
#                          result_stream_args=premium_search_args)

# len(tweets_2)

# [print(tweet.all_text, end='\n\n') for tweet in tweets_2[0:10]]

# tweets_data = pd.DataFrame(tweets)

# type(tweets[3]["quoted_status"]) == type({})
# tweets_data_2

# nested_columns = []
# for tweet in tweets[:2]:
#     print("tweet {}".format(tweets.index(tweet)))
#     for col in tweet.keys():
        
#         if tweet[col] and (type(tweet[col]) == type({}) or type(tweet[col]) == type([])):
#             print("\tcolumn: {}\tvalue: {}".format(col, tweet[col]))

# # all_text = tweets_data["text"].drop_duplicates()
# all_text = pd.Series([tweet.all_text for tweet in tweets])
# all_text.drop_duplicates(inplace=True)

# # import codecs
# # codecs.getdecoder("unicode_escape")(all_text.loc[8])[0]
# all_text_2

# all_text_refined = all_text.apply(lambda s: re.subn('(\s)', ' ', s)[0])

# all_text_refined.loc[454]

# match_str = '(government collapse)'
# reqd_df = all_text_refined[all_text_refined.str.contains(match_str)]
# # reqd_df.drop_duplicates(subset=['headline_text'], inplace=True)

# reqd_df_2

# def combine_data_offline(tweets_files, news_files, prev_files_map=None):
#     processed_headlines = processed_tweets = prev_data_df = pd.DataFrame()
    
#     if prev_files_map:
#         for filename, column in prev_files_map.items():
#             file_df = get_previous_data(filename, column)
#             if prev_data_df.empty:
#                 prev_data_df = file_df
#             else:
#                 prev_data_df = prev_data_df.append(file_df)
#         prev_data_df = prev_data_df.reset_index().drop(columns=["index"])
#         prev_data_df.rename(columns={prev_data_df.columns[0]: "text"}, 
#                             inplace=True)
#         prev_data_df.drop_duplicates(inplace=True)
#     processed_tweets = pd.read_csv(os.path.join(tw_output_directory,
#                                                 "tweets_2019-11-11_11-41.csv"),
#                                    encoding="UTF-8")
    
#     tw_text = list(processed_tweets["text"])
#     processed_headlines = pd.read_csv(os.path.join(nh_output_directory, 
#                                           "news_2019-11-11_11-41.csv"),
#                              encoding="UTF-8")
#     nh_text = list(processed_headlines["url"])
#     tw_text.extend(nh_text)
#     if not prev_data_df.empty:
#         base_ = list(prev_data_df["text"])
#         base_.extend(tw_text)
#     else:
#         base_ = tw_text
#     final_df = pd.DataFrame(base_, columns=["data"])
#     return final_df